---
title: "AI for Automatic Synoptic Reporting"
aliases:
  - cap_forms_results.html
execute:
  echo: false
---

### CAP Forms Description
CAP (College of American Pathologists) forms are standardized cancer reporting protocols that have revolutionized pathology practice by replacing inconsistent narrative reports with structured, synoptic formats containing essential diagnostic and prognostic information. Developed over 35 years ago to address significant variability in cancer reporting, these evidence-based protocols ensure complete, uniform documentation of malignant tumors across all healthcare institutions, directly improving patient outcomes and clinical decision-making. 

Lung resection CAP forms are particularly critical in thoracic oncology, providing standardized reporting templates for primary lung cancers that include essential elements such as tumor size, histologic type and grade, surgical margins, lymph node status, and staging classifications. These lung-specific protocols have demonstrated measurable clinical impact, with studies showing that synoptic reporting achieves 88.4% completeness compared to only 2.6% for traditional descriptive reports, leading to more accurate staging, better treatment planning, and improved survival rates.

By establishing consistent terminology and data capture requirements, CAP lung resection forms enhance communication between pathologists and oncologists, ensure regulatory compliance with Commission on Cancer standards, and provide the structured data foundation necessary for personalized cancer care, targeted therapy selection, and multidisciplinary treatment coordination. The widespread adoption of these standardized protocols, supported by electronic integration into laboratory information systems, has positioned pathologists as key members of the lung cancer care team while enabling seamless data exchange for cancer registries, research, and quality improvement initiatives.

### CAP Forms Dataset Description

At Stanford, CAP forms were implemented within Epic using SmartForms. SmartForms is an Epic product that allows the capture of semi-structured information within the EHR that can later be used to generate free-form reports. In this case, the structured information from the CAP forms is captured on SmartForms, which later generate the synoptic reporting section within the pathology report. As a consequence, for relevant cases, the pathology report will contain this section.

For this task, we aim to use AI to populate the CAP forms automatically by using all the other elements from the pathology report. We formulated this as a question-answering problem, where the context for each question is the entire pathology report, the question is the particular CAP form element, and the answer is the value/selection to be populated.

For this initial experiment, we collected a dataset with 609 patients and 615 lung resection forms. The forms were reported between November 2022 and March 2025. The table below summarizes the demographics for this dataset. It is important to notice that this a silver standard dataset, since no actual annotations from Pathologist were made to confirm that the rest of the pathology report contained sufficient information to populate the associated CAP form. 

### Demographic Analysis 

```{r, message=FALSE, warning=FALSE, results='hide'}
library(DBI)
library(bigrquery)
library(glue)
library(plotly)
library(dplyr)
library(gt)
library(table1)

credentials_path <- "/home/rstudio/.config/gcloud/application_default_credentials.json"
project_id <- "som-rit-phi-oncology-dev"
Sys.setenv(GOOGLE_APPLICATION_CREDENTIALS = credentials_path)

fetch_data_from_sql <- function(sql) {
    # Connect to BigQuery

    conn <- dbConnect(
        bigrquery::bigquery(),
        project = project_id,
        use_legacy_sql = FALSE
    )

    on.exit(dbDisconnect(conn), add = TRUE)

    # Run query
    result <- tryCatch(
        {
            dbGetQuery(conn, sql)
        },
        error = function(e) {
            message(glue("\n⚠️ Query failed: {e$message}"))
            return(NULL)
        }
    )

    # Return the result as a single data frame
    return(result)
}
```

```{r, message=FALSE, warning=FALSE, results='hide'}
sql <- "
with identifiers as (
          select distinct stanford_patient_uid, 
                         
                          max(taken_time) as event_date 
         from 
           `som-rit-phi-oncology-prod.oncology_cap_forms_phi_irb76049_aug2025.cap_forms`
         group by stanford_patient_uid
           ),
t1 as (
SELECT 
    person.person_source_value AS stanford_patient_uid,
    DATE_DIFF(CAST(event_date AS DATE), CAST(person.birth_datetime AS DATE), YEAR) AS age,
    race_concept.concept_name AS race,
    ethnicity_concept.concept_name as ethnicity,
    gender_concept.concept_name AS sex
FROM
 `som-rit-phi-oncology-prod.oncology_omop_phi_irb76049_aug2025.person` AS person
JOIN identifiers
ON
    person.person_source_value = identifiers.stanford_patient_uid
JOIN
  `som-rit-phi-oncology-prod.oncology_omop_phi_irb76049_aug2025.concept` AS race_concept
ON
 person.race_concept_id = race_concept.concept_id
JOIN
    `som-rit-phi-oncology-prod.oncology_omop_phi_irb76049_aug2025.concept` AS ethnicity_concept
ON
 person.ethnicity_concept_id = ethnicity_concept.concept_id
JOIN
    `som-rit-phi-oncology-prod.oncology_omop_phi_irb76049_aug2025.concept` AS gender_concept
ON
 person.gender_concept_id = gender_concept.concept_id
 )
SELECT
    sex as Sex,
    CASE 
      WHEN race = 'No matching concept' THEN 'Unknown'
      ELSE race
    END AS Race,
    CASE 
      WHEN ethnicity = 'No matching concept' THEN 'Unknown'
      ELSE ethnicity
    END as Ethnicity,
    CASE
      WHEN age < 18 THEN '0-17'
      WHEN age >= 18
    AND age < 45 THEN '18-44'
      WHEN age >= 45 AND age < 60 THEN '45-59'
      WHEN age >= 60 AND age < 70 THEN '60-69'
      WHEN age >= 70 AND age < 80 THEN '70-79'
      WHEN age >= 80 THEN '80+'
      ELSE 'Unknown'
  END
    AS Age
FROM
    t1
"

df_demographics <- fetch_data_from_sql(sql)
```

```{r, message=FALSE} 
table1(~ Age + Race + Ethnicity | Sex , df_demographics)
```


## Methods and Results

For this experiment, we used several state-of-the-art LLMs to assess their capabilities to extract the information required for lung resection CAP forms. As mentioned before, we formulated the problem as a question-answering problem. The full set of top level questions for the Lung Resection CAp forms alongside the descriptions for each one of them can be found [here](cap_forms_appendix.qmd#capforms-toplevel-questions). Each LLM was tested in a zero-shot setting, where no actual examples were given. Each LLM was asked to answer a single question using the entire pathology report (excluding the synoptic report) and instructions that contained the most recent lung resection CAP forms instructions. We asked the LLMs to provide the answers using JSON to facilitate the parsing of the actual answers. The prompt used can be found [here](cap_forms_appendix.qmd#capforms-zeroshot-prompt)

To automatically evaluate the output, we used the traditional BERT score, which evaluates the semantic similarity between the generated answer and the reference answer. The results of this evaluation are shown in the figure below.


Distribution of average BERT scores across models is shown below. Histograms with dashed median lines indicate a minor differences in central tendency and variability between models. Summary statistics (mean, median, quartiles) are provided as follow.

```{r, warning=FALSE, message=FALSE}
library(plotly)
library(dplyr)
library(readr)
library(RColorBrewer)
library(tidyr)
library(gt)

# Read the CSV file
df_plot <- read_csv("/workspaces/starr-oncology-data-lake-arpah/src/aug_2025/data/cap_forms_model_performance_comparison.csv")



 model_stats <- df_plot %>%
   group_by(model_name) %>%
   summarise(
     mean = round(mean(avg_score, na.rm = TRUE), 2),
     median = round(median(avg_score, na.rm = TRUE), 2),
     q1 = round(quantile(avg_score, 0.25, na.rm = TRUE), 2),
     q3 = round(quantile(avg_score, 0.75, na.rm = TRUE), 2),
     n = n()
   )
 
 
 p <- ggplot(df_plot, aes(x = avg_score, fill = model_name)) +
   # histogram (counts)
   geom_histogram(aes(y = ..count..), bins = 25, color = "black", alpha = 0.4, position = "identity") +
   # density curve scaled to counts
   geom_density(aes(y=..count..*25), color = "grey", size = 1, alpha=0.7) +
   # median line with legend
   geom_vline(data = model_stats, aes(xintercept = median, color = "Median"), linetype = "dashed", size = 1) +
   # add median value as text
   geom_text(data = model_stats, aes(x = median, y = Inf, label = paste0(median)),
             color = "darkred", vjust = 2, hjust = -0.99, inherit.aes = FALSE, size = 3) +
   facet_wrap(~model_name, scales = "free_y", ncol=1) +
   theme_minimal() +
   scale_fill_brewer(palette = "Set2") +
   scale_color_manual(name = "", values = c("Median" = "darkred")) +  # legend for median
   labs(
     title = "Model Distribution",
     x = "Average BERT Score",
     y = "Count",
     fill = "Model Name"
   )
 
# Summary table
model_stats_table <- model_stats %>%
  knitr::kable(caption = "Summary Statistics per Model")

# Display
print(p)
model_stats_table
```

To enable structured evaluation, individual questions from the lung resection CAP forms were grouped into high-level categories based on clinical relevance and reporting conventions. 
The distributions of average BERT score for each model were visualized using overlaid histograms with dashed lines indicating the median score per model within each category.
Across categories, Gemini generally exhibits the highest median scores and narrow distributions, indicating consistently strong performance. Llama4 shows slightly lower medians with moderate variability, while OpenAI_OSS is more variable across tasks but performs well in certain categories such as Specimen & Procedure. Category-specific differences highlight that some tasks, like Local Extension & Invasion, are easier for all models, whereas Treatment Effect and Tumor Histology & Subtypes remain more challenging.
To facilitate analysis, we organized individual CAP form elements into high-level categories based on clinical and reporting similarity. Each category represents a logical grouping of related questions that capture similar aspects of lung resection pathology. This approach allowed us to assess model performance at both the individual-question and category level (e.g., how well LLMs capture margin information overall) and facilitated clearer identification of domains where AI assistance may be most beneficial to pathologists (e.g., strong performance in Margins vs. weaker performance in Histologic Grade). The categories were defined as follows:

- **Specimen & Procedure**: Information about the surgical specimen and procedure, including specimen ID, laterality, and general notes.
- **Tumor Size, Focality, Distribution**: Details on tumor dimensions, number of nodules, total and invasive sizes, focality, and presence of synchronous tumors.
- **Tumor Histology & Subtypes**: Histologic type, grade, patterns, components, and any descriptive comments from the pathologist.
- **Local Extension & Invasion**: Extent of tumor spread, including invasion of adjacent structures, visceral pleura, lymphovascular spaces, and STAS.
- **Lymph Nodes**: Nodal involvement, number of nodes examined and positive, prior lymph node procedures, and extranodal extension.
- **Margins**: Status and distance of surgical margins (bronchial, vascular, parenchymal, pleural) and comments.
- **Staging**: AJCC/UICC TNM descriptors and categories (pT, pN, pM) for pathologic staging.
- **Treatment Effect**: Evidence of therapy response, including necrosis, viable tumor, and stromal changes.
- **Metadata**: Additional findings and miscellaneous notes not captured in other categories.



## Lung CAP Form Questions {.tabset}

### Specimen & Procedure
Information about the surgical specimen and procedure, including specimen ID, laterality, and general notes.
- LUNG || SPECIMEN ID(S)  
- PROCEDURE  
- SPECIMEN LATERALITY  
- NOTE  

### Tumor Size, Focality, Distribution
Details on tumor dimensions, number of nodules, total and invasive sizes, focality, and presence of synchronous tumors.
- LUNG || ADDITIONAL DIMENSION (CENTIMETERS)  
- LUNG || NUMBER OF INTRAPULMONARY METASTASES  
- LUNG || TOTAL NUMBER OF PRIMARY TUMORS  
- TOTAL TUMOR SIZE (SIZE OF ENTIRE TUMOR)  
- SIZE OF INVASIVE COMPONENT (REQUIRED ONLY IF INVASIVE NONMUCINOUS ADENOCARCINOMAS WITH LEPIDIC COMPO)  
- TUMOR FOCALITY  
- NUMBER OF TUMOR NODULES  
- SYNCHRONOUS TUMORS (REQUIRED IF MORPHOLOGICALLY DISTINCT UNRELATED MULTIPLE PRIMARY TUMORS ARE PRESENT)  
- TUMOR SITE  
- PERCENTAGE OF TOTAL TUMOR SIZE (ABOVE)  

### Tumor Histology & Subtypes
Histologic type, grade, patterns, components, and any descriptive comments from the pathologist.
- HISTOLOGIC TYPE (NOTE A)  
- HISTOLOGIC TYPE (NOTE C)  
- LUNG || HISTOLOGIC TYPE COMMENT  
- HISTOLOGIC GRADE  
- HISTOLOGIC PATTERNS (MAY INCLUDE PERCENTAGES)  
- HISTOLOGIC COMPONENT(S) PRESENT (MAY INCLUDE PERCENTAGES)  
- LUNG || COMMENT(S)  

### Local Extension & Invasion
Nodal involvement, number of nodes examined and positive, prior lymph node procedures, and extranodal extension.
- DIRECT INVASION OF ADJACENT STRUCTURES (NOTE G)  
- DIRECT INVASION OF OTHER STRUCTURES  
- INVOLVED ADJACENT STRUCTURES  
- VISCERAL PLEURA INVASION  
- LYMPHOVASCULAR INVASION  
- LYMPHOVASCULAR INVASION (NOTE F)  
- SPREAD THROUGH AIR SPACES (STAS)  

### Regional Lymph Nodes & Distant Metastis
Nodal involvement, number of nodes examined and positive, prior lymph node procedures, and extranodal extension
- LUNG || PRIOR LYMPH NODE PROCEDURE(S) INCLUDED  
- LUNG || REGIONAL LYMPH NODE COMMENT  
- LYMPH NODE(S) FROM PRIOR PROCEDURES  
- NUMBER OF LYMPH NODES EXAMINED  
- NUMBER OF LYMPH NODES WITH TUMOR  
- REGIONAL LYMPH NODE STATUS  
- REGIONAL LYMPH NODES  
- REGIONAL LYMPH NODES (PN)  
- NODAL SITE(S) EXAMINED  
- NODAL SITE(S) WITH TUMOR  
- EXTRANODAL EXTENSION  

### Margins
Status and distance of surgical margins (bronchial, vascular, parenchymal, pleural) and comments
- CLOSEST MARGIN(S) TO INVASIVE CARCINOMA  
- DISTANCE FROM INVASIVE CARCINOMA TO CLOSEST MARGIN  
- LUNG || MARGIN COMMENT  
- MARGIN STATUS FOR INVASIVE CARCINOMA  
- MARGIN STATUS FOR NON-INVASIVE TUMOR  
- MARGIN(S) INVOLVED BY INVASIVE CARCINOMA  

### Staging (AJCC/UICC)
AJCC/UICC TNM descriptors and categories (pT, pN, pM) for pathologic staging
- PRIMARY TUMOR (PT)  
- PN CATEGORY  
- PM CATEGORY (REQUIRED ONLY IF CONFIRMED PATHOLOGICALLY)  
- TNM DESCRIPTORS  

### Treatment Effect
Evidence of therapy response, including necrosis, viable tumor, and stromal changes
- TREATMENT EFFECT  
- TREATMENT EFFECT (NOTE I)  
- PERCENTAGE OF NECROSIS  
- PERCENTAGE OF RESIDUAL VIABLE TUMOR  
- PERCENTAGE OF STROMA (INCLUDES FIBROSIS AND INFLAMMATION)  

### Additional Findings 
Additional findings and miscellaneous notes not captured in other categories
- ADDITIONAL FINDINGS




```{r message=FALSE, warning=FALSE, include=FALSE}
library(plotly)
library(dplyr)
library(readr)
library(RColorBrewer)
library(tidyr)
library(gt)

# Read the CSV file
df_plot <- read_csv("/workspaces/starr-oncology-data-lake-arpah/src/aug_2025/data/cap_forms_model_performance_comparison.csv")

# Calculate statistics for each model
model_stats <- df_plot %>%
  group_by(model_name) %>%
  summarise(
    mean = round(mean(avg_score, na.rm = TRUE), 3),
    median = round(median(avg_score, na.rm = TRUE), 3),
    q1 = round(quantile(avg_score, 0.25, na.rm = TRUE), 3),
    q3 = round(quantile(avg_score, 0.75, na.rm = TRUE), 3),
    min_val = round(min(avg_score, na.rm = TRUE), 3),
    max_val = round(max(avg_score, na.rm = TRUE), 3),
    .groups = 'drop'
  ) %>%
  mutate(
    iqr = round(q3 - q1, 3)
  )

# Get unique models for consistent ordering
unique_models <- unique(df_plot$model_name)

# Create enhanced color palette
n_models <- length(unique_models)
if(n_models <= 8) {
  colors <- RColorBrewer::brewer.pal(max(3, n_models), "Set2")
} else if(n_models <= 12) {
  colors <- RColorBrewer::brewer.pal(12, "Set3")
} else {
  colors <- rainbow(n_models, s = 0.7, v = 0.8)  # More muted rainbow
}

# Ensure we have enough colors
if(length(colors) < n_models) {
  colors <- rep(colors, ceiling(n_models/length(colors)))[1:n_models]
}

# Initialize empty plot
p <- plot_ly()

# Add a trace for each model with different colors
for(i in 1:length(unique_models)) {
  model <- unique_models[i]
  model_data <- df_plot %>% filter(model_name == model)
  model_stat <- model_stats %>% filter(model_name == model)
  
  # Create custom hover template with statistics
  hover_template <- paste0(
    "<b>Model:</b> %{x}<br>",
    "<b>Value:</b> %{y}<br>",
    "<b>Statistics:</b><br>",
    "Mean: ", model_stat$mean, "<br>",
    "Median: ", model_stat$median, "<br>",
    "Q1: ", model_stat$q1, "<br>",
    "Q3: ", model_stat$q3, "<br>",
    "IQR: ", model_stat$iqr, "<br>",
    "Min: ", model_stat$min_val, "<br>",
    "Max: ", model_stat$max_val,
    "<extra></extra>"
  )
  
  p <- p %>% add_trace(
    data = model_data,
    x = ~model_name,
    y = ~avg_score,
    type = "box",
    name = model,
    boxpoints = "outliers",
    marker = list(
      size = 4,
      opacity = 0.7,
      color = colors[i]
    ),
    line = list(
      width = 2,
      color = colors[i]
    ),
    fillcolor = paste0(colors[i], "60"),  # Add transparency
    boxmean = FALSE,
    notched = TRUE,
    jitter = 0.3,
    pointpos = 0,
    hovertemplate = hover_template,
    hoverlabel = list(
      bgcolor = colors[i],
      font = list(color = "white", size = 12)
    )
  )
}

# Customize layout with enhanced styling
p <- p %>% layout(
  title = list(
    text = 'BERT F1-Score Distribution by Model',
    font = list(size = 20, family = "Arial Black"),
    x = 0.5,
    y = 0.95
  ),
  
  xaxis = list(
    title = list(
      text = '',
      font = list(size = 14, family = "Arial")
    ),
    tickfont = list(size = 11, family = "Arial"),
    tickangle = 45,
    showgrid = TRUE,
    gridwidth = 1,
    gridcolor = 'rgba(128,128,128,0.2)',
    zeroline = FALSE,
    showline = TRUE,
    linewidth = 2,
    linecolor = 'rgba(128,128,128,0.8)',
    type = 'category'  # Ensures only model names appear on x-axis
  ),
  
  yaxis = list(
    title = list(
      text = 'BERT f1 Score',  # Updated y-axis title
      font = list(size = 14, family = "Arial")
    ),
    tickfont = list(size = 11, family = "Arial"),
    showgrid = TRUE,
    gridwidth = 1,
    gridcolor = 'rgba(128,128,128,0.2)',
    zeroline = TRUE,
    zerolinewidth = 2,
    zerolinecolor = 'rgba(128,128,128,0.4)',
    showline = TRUE,  # Fixed: was "True" instead of TRUE
    linewidth = 2,
    linecolor = 'rgba(128,128,128,0.8)'
  ),
  
  plot_bgcolor = '#fafafa',
  paper_bgcolor = 'white',
  
  showlegend = FALSE,
  
  margin = list(
    l = 80,
    r = 60,
    b = 120,
    t =100
  )
)
# Display the plot

p
```


```{r, warning=FALSE, message=FALSE}

library(plotly)
library(dplyr)
library(readr)
library(RColorBrewer)
library(tidyr)
library(gt)

# Read the CSV file
df_plot <- read_csv("/workspaces/starr-oncology-data-lake-arpah/src/aug_2025/data/cap_forms_model_performance_comparison.csv")
df_plot <- df_plot %>%
  mutate(category = case_when(
    # Specimen & Procedure
    grepl("SPECIMEN ID|PROCEDURE|LATERALITY|NOTE", question, ignore.case = TRUE) ~ "Specimen & Procedure",
    
    # Tumor Size, Focality, Distribution
    grepl("ADD(ITIONAL)? DIMENSION|NUMBER OF INTRAPULMONARY METASTASES|TOTAL NUMBER OF PRIMARY TUMORS|TOTAL TUMOR SIZE|SIZE OF INVASIVE COMPONENT|FOCALITY|NUMBER OF TUMOR NODULES|SYNCHRONOUS|TUMOR SITE|PERCENTAGE OF TOTAL TUMOR SIZE", 
          question, ignore.case = TRUE) ~ "Tumor Size, Focality, Distribution",
    
    # Tumor Histology & Subtypes
    grepl("HISTOLOGIC TYPE|HISTOLOGIC GRADE|HISTOLOGIC PATTERNS|HISTOLOGIC COMPONENT|COMMENT\\(S\\)|LUNG \\|\\| HISTOLOGIC TYPE COMMENT", 
          question, ignore.case = TRUE) ~ "Tumor Histology & Subtypes",
    
    # Local Extension & Invasion
    grepl("INVASION|INVOLVED ADJACENT|LYMPHOVASCULAR|EXTRANODAL|SPREAD THROUGH AIR SPACES|VISCERAL PLEURA", question, ignore.case = TRUE) ~ "Local Extension & Invasion",
    
    # Lymph Nodes
    grepl("LYMPH NODE|NODAL|DISTANT SITE", question, ignore.case = TRUE) ~ "Regional Lymph Nodes & Distant Metastasis",
    
    # Margins
    grepl("MARGIN", question, ignore.case = TRUE) ~ "Margins",
    
    # Staging
    grepl("\\bPT\\b|PN CATEGORY|PM CATEGORY|TNM DESCRIPTORS", question, ignore.case = TRUE) ~ "Staging",
    
    # Treatment Effect
    grepl("TREATMENT EFFECT|NECROSIS|RESIDUAL VIABLE TUMOR|STROMA", question, ignore.case = TRUE) ~ "Treatment Effect",
    
    # Metadata / Additional Findings
    grepl("ADDITIONAL FINDINGS", question, ignore.case = TRUE) ~ "Metadata",
    
    # Default
    TRUE ~ "Uncategorized"
  ))

```

```{r, warning=FALSE, message=FALSE, include=FALSE}

library(ggplot2)
library(dplyr)

# Compute medians per model
median_cats <- df_plot %>%
  group_by(model_name, category) %>%
  summarise(median_score = median(avg_score))
library(ggplot2)
library(dplyr)


  
  # plot with facetting (one facet per category)
  pg <- ggplot(df_plot, aes(x = avg_score, fill = model_name)) +
    geom_histogram(aes(y = ..count..), bins = 35, color = "black", alpha = 0.5, position = "identity") +
    #geom_density(aes(y = ..count.. * 25), color = "grey", size = 1, alpha = 0.7) +
    geom_vline(data = median_cats, aes(xintercept = median_score, color = model_name),
               linetype = "dashed", size = 0.5) +
               geom_vline(aes(xintercept = 0.85, linetype = "Threshold"), color = "darkred", size = 0.7) +   # reference line
    scale_fill_brewer(palette = "Set2") +
    scale_color_brewer(palette = "Set2") +
    theme_minimal() +
    labs(
      title = "Distribution of Avg BERT Scores across Categories",
       subtitle ="",
        caption = "Solid red line = reference threshold at 0.85 \n  Dashed lines = model-specific medians",
      x = "Average BERT F1-Score",
      y = "Count",
      fill = "Model Name",
      color = "Model Name"
    ) +
    facet_wrap(~category)+theme(
    plot.margin = margin(t = 20, r = 20, b = 20, l = 20),   # top, right, bottom, left
    strip.text = element_text(size = 11, face = "bold")     # optional: clearer facet labels
  )+
   scale_linetype_manual(
    name = "Reference Lines",
    values = c("Median" = "dashed", "Threshold" = "solid")
  ) 
  

pg

```
```{r, warning=FALSE, message=FALSE}
 model_stats_grp <- df_plot %>%
   group_by(model_name, category) %>%
   summarise(
     mean = round(mean(avg_score, na.rm = TRUE), 2),
     median = round(median(avg_score, na.rm = TRUE), 2),
     q1 = round(quantile(avg_score, 0.25, na.rm = TRUE), 2),
     q3 = round(quantile(avg_score, 0.75, na.rm = TRUE), 2),
     n = n()
   )



# Create interactive datatable
DT::datatable(
  model_stats_grp,
  rownames = FALSE
)
```
```{r, warning=FALSE, message=FALSE}
datT=df_plot %>% filter(category %in% c("Tumor Histology & Subtypes", "Tumor Size, Focality, Distribution"))
# boxplot
p_box <- ggplot(datT, aes(x = model_name, y = avg_score, fill = model_name)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  labs(
    title = "Distribution of BERT F1-Score for Tumor Characteristics",
     subtitle = "Dashed lines = model-specific medians",
    caption = "Solid red line = reference threshold at 0.85",
    x = "Model Name",
    y = "Average BERT F1-Score",
    fill = "Model Name"
  ) +
  facet_wrap(~category)

p_box

```

### Model Performance 

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
source("/workspaces/starr-oncology-data-lake-arpah/src/R/all_function.R", encoding = "UTF-8")

# Prepare table
df_summary <- df_plot %>%
  group_by(model_name, category) %>%
  summarise(median_score = median(avg_score)) %>%
  pivot_wider(names_from = model_name, values_from = median_score) %>%
  rowwise() %>%
  mutate(best_model = names(.)[which.max(c_across(-category)) + 1])  # +1 because first column is category

df_summary

```

```{r,warning=FALSE, message=FALSE}
library(ggplot2)
library(plotly)
library(dplyr)

# Compute median per model-category
df_median <- df_plot %>%
  group_by(model_name, category) %>%
  summarise(median_score = median(avg_score)) %>%
  ungroup()

# Prepare matrix for heatmap
heatmap_matrix <- df_median %>%
  tidyr::pivot_wider(names_from = model_name, values_from = median_score) %>%
 tibble:: column_to_rownames("category") %>%
  as.matrix()

# Interactive heatmap
plot_ly(
  z = heatmap_matrix,
  x = colnames(heatmap_matrix),
  y = rownames(heatmap_matrix),
  type = "heatmap",
  colorscale = list(c(0,1), c("white", "darkred")), # black → white
  reversescale = FALSE
) %>%
  layout(
    title = "Interactive Heatmap of Median Scores by Model and Category",
    xaxis = list(title = "Model", tickfont = list(color = "white")),
    yaxis = list(title = "Category", tickfont = list(color = "white")),
    paper_bgcolor = "black",
    plot_bgcolor = "black",
    font = list(color = "white")
  )

plot_ly(
  z = heatmap_matrix,
  x = colnames(heatmap_matrix),
  y = rownames(heatmap_matrix),
  type = "heatmap",
  colorscale = "Viridis",    # perceptually uniform, works on dark background
  reversescale = FALSE,
  zmin = 0, zmax = 1
) %>%
  layout(
    title = "Interactive Heatmap of Median Scores by Model and Category",
    xaxis = list(title = "Model", tickangle = -45, tickfont = list(color = "white")),
    yaxis = list(title = "Category", tickfont = list(color = "white")),
    paper_bgcolor = "black",
    plot_bgcolor = "black",
    font = list(color = "white")
  )

```

```{r ,warning=FALSE, message=FALSE}
library(plotly)
library(dplyr)

# Compute median per model-category
df_median <- df_plot %>%
  group_by(model_name, category) %>%
  summarise(median_score = median(avg_score)) %>%
  ungroup()

# Bubble plot
plot_ly(
  df_median,
  x = ~model_name,
  y = ~category,
  size = ~median_score,
  color = ~median_score,
  colors = colorRamp(c("darkred", "lightyellow", "darkgreen")),  # low → high
  text = ~paste("Category:", category, "<br>Model:", model_name, "<br>Median Score:", round(median_score,3)),
  hoverinfo = "text",
  type = "scatter",
  mode = "markers"
) %>%
  layout(
    title = "Bubble Plot of Median Scores by Model and Category",
    xaxis = list(title = "Model", tickfont = list(color = "white")),
    yaxis = list(title = "Category", tickfont = list(color = "white")),
    paper_bgcolor = "black",
    plot_bgcolor = "black",
    font = list(color = "white")
  )

```
In this evaluation we only use 3 models vs the 5 presented before. We choose to use the open source version of GPT to favor open source models with more recent data cuts. It has been shown that this model is particularly good at health related tasks. All models were use through Google Cloud using their Vertex AI HIPAA compliant deployments and endpoints. The models endpoints used were: `openai/gpt-oss-20b-maas` for OpenAI OSS, `gemini-2.5-flash` for the latest Gemini model and `meta/llama-4-scout-17b-16e-instruct-maas` for the latest LLAMA model.  

In this graph we can see that the best overall model is Gemini 2.5 with a median of 0.85, followed by LLama 4 with 0.83 and OpenAI OSS with 0.81. 

We also attempted an experiment were we used LLM a Judge to verify that the answers were aligned with the reference answer from the CAP form. At this moment that work requires more experimentation and refinement using Pathologist expert annotations to ensure the Judging process can be used as an objective measure of performance. 