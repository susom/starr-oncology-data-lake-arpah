---
title: "AI for Automatic Synoptic Reporting"
aliases:
  - cap_forms_results.html
execute:
  echo: false
---

CAP (College of American Pathologists) forms are standardized cancer reporting protocols that have revolutionized pathology practice by replacing inconsistent narrative reports with structured, synoptic formats containing essential diagnostic and prognostic information. Developed over 35 years ago to address significant variability in cancer reporting, these evidence-based protocols ensure complete, uniform documentation of malignant tumors across all healthcare institutions, directly improving patient outcomes and clinical decision-making. 

Lung resection CAP forms are particularly critical in thoracic oncology, providing standardized reporting templates for primary lung cancers that include essential elements such as tumor size, histologic type and grade, surgical margins, lymph node status, and staging classifications. These lung-specific protocols have demonstrated measurable clinical impact, with studies showing that synoptic reporting achieves 88.4% completeness compared to only 2.6% for traditional descriptive reports, leading to more accurate staging, better treatment planning, and improved survival rates.

By establishing consistent terminology and data capture requirements, CAP lung resection forms enhance communication between pathologists and oncologists, ensure regulatory compliance with Commission on Cancer standards, and provide the structured data foundation necessary for personalized cancer care, targeted therapy selection, and multidisciplinary treatment coordination. The widespread adoption of these standardized protocols, supported by electronic integration into laboratory information systems, has positioned pathologists as key members of the lung cancer care team while enabling seamless data exchange for cancer registries, research, and quality improvement initiatives.

At Stanford, CAP forms were implemented within Epic using SmartForms. SmartForms is an Epic product that allows the capture of semi-structured information within the EHR that can later be used to generate free-form reports. In this case, the structured information from the CAP forms is captured on SmartForms, which later generate the synoptic reporting section within the pathology report. As a consequence, for relevant cases, the pathology report will contain this section.

For this task, we aim to use AI to populate the CAP forms automatically by using all the other elements from the pathology report. We formulated this as a question-answering problem, where the context for each question is the entire pathology report, the question is the particular CAP form element, and the answer is the value/selection to be populated.

For this initial experiment, we collected a dataset with 390 patients and 390 lung resection forms. The forms were reported between November 2022 and April 2025. The table below summarizes the demographics for this dataset.


```{r, message=FALSE, warning=FALSE, results='hide'}
library(DBI)
library(bigrquery)
library(glue)
library(plotly)
library(dplyr)
library(gt)

credentials_path <- "/home/rstudio/.config/gcloud/application_default_credentials.json"
project_id <- "som-rit-phi-oncology-dev"
Sys.setenv(GOOGLE_APPLICATION_CREDENTIALS = credentials_path)

fetch_data_from_sql <- function(sql) {
    # Connect to BigQuery

    conn <- dbConnect(
        bigrquery::bigquery(),
        project = project_id,
        use_legacy_sql = FALSE
    )

    on.exit(dbDisconnect(conn), add = TRUE)

    # Run query
    result <- tryCatch(
        {
            dbGetQuery(conn, sql)
        },
        error = function(e) {
            message(glue("\n⚠️ Query failed: {e$message}"))
            return(NULL)
        }
    )

    # Return the result as a single data frame
    return(result)
}
```

```{r, message=FALSE, warning=FALSE, results='hide'}
sql <- "
with identifiers as (
          select distinct stanford_patient_uid, 
                          order_proc_id, 
                          max(event_date) as event_date 
         from 
           `som-rit-phi-oncology-dev.cap_forms.final_train_data_identifiers`
         group by stanford_patient_uid, order_proc_id
           ),
t1 as (
SELECT 
    person.person_source_value AS stanford_patient_uid,
    DATE_DIFF(CAST(event_date AS DATE), CAST(person.birth_datetime AS DATE), YEAR) AS age,
    race_concept.concept_name AS race,
    ethnicity_concept.concept_name as ethnicity,
    gender_concept.concept_name AS sex
FROM
 `som-rit-phi-oncology-prod.oncology_omop_phi_irb76049_aug2025.person` AS person
JOIN identifiers
ON
    person.person_source_value = identifiers.stanford_patient_uid
JOIN
  `som-rit-phi-oncology-prod.oncology_omop_phi_irb76049_aug2025.concept` AS race_concept
ON
 person.race_concept_id = race_concept.concept_id
JOIN
    `som-rit-phi-oncology-prod.oncology_omop_phi_irb76049_aug2025.concept` AS ethnicity_concept
ON
 person.ethnicity_concept_id = ethnicity_concept.concept_id
JOIN
    `som-rit-phi-oncology-prod.oncology_omop_phi_irb76049_aug2025.concept` AS gender_concept
ON
 person.gender_concept_id = gender_concept.concept_id
 )
SELECT
    sex as Sex,
    CASE 
      WHEN race = 'No matching concept' THEN 'Unknown'
      ELSE race
    END AS Race,
    CASE 
      WHEN ethnicity = 'No matching concept' THEN 'Unknown'
      ELSE ethnicity
    END as Ethnicity,
    CASE
      WHEN age < 18 THEN '0-17'
      WHEN age >= 18
    AND age < 45 THEN '18-44'
      WHEN age >= 45 AND age < 60 THEN '45-59'
      WHEN age >= 60 AND age < 70 THEN '60-69'
      WHEN age >= 70 AND age < 80 THEN '70-79'
      WHEN age >= 80 THEN '80+'
      ELSE 'Unknown'
  END
    AS Age
FROM
    t1
"

df_demographics <- fetch_data_from_sql(sql)
```

```{r}
library(table1)  
table1(~ Age + Race + Ethnicity | Sex , df_demographics)
```


## Methods and Results

For this experiment, we used several state-of-the-art LLMs to assess their capabilities to extract the information required for lung resection CAP forms. As mentioned before, we formulated the problem as a question-answering problem. Each LLM was tested in a zero-shot setting, where no actual examples were given. Each LLM was asked to answer a single question using the entire pathology report (excluding the synoptic report) and instructions that contained the most recent lung resection CAP forms instructions. We asked the LLMs to provide the answers using JSON to facilitate the parsing of the actual answers.

To automatically evaluate the output, we used the traditional BERT score, which evaluates the semantic similarity between the generated answer and the reference answer. The results of this evaluation are shown in the figure below.

```{r message=FALSE, warning=FALSE}
library(plotly)
library(dplyr)
library(readr)
library(RColorBrewer)
library(tidyr)
library(gt)

# Read the CSV file
df_plot <- read_csv("/workspaces/starr-oncology-data-lake-arpah/src/aug_2025/data/cap_forms_model_performance_comparison.csv")

# Calculate statistics for each model
model_stats <- df_plot %>%
  group_by(model_name) %>%
  summarise(
    mean = round(mean(avg_score, na.rm = TRUE), 3),
    median = round(median(avg_score, na.rm = TRUE), 3),
    q1 = round(quantile(avg_score, 0.25, na.rm = TRUE), 3),
    q3 = round(quantile(avg_score, 0.75, na.rm = TRUE), 3),
    min_val = round(min(avg_score, na.rm = TRUE), 3),
    max_val = round(max(avg_score, na.rm = TRUE), 3),
    .groups = 'drop'
  ) %>%
  mutate(
    iqr = round(q3 - q1, 3)
  )

# Get unique models for consistent ordering
unique_models <- unique(df_plot$model_name)

# Create enhanced color palette
n_models <- length(unique_models)
if(n_models <= 8) {
  colors <- RColorBrewer::brewer.pal(max(3, n_models), "Set2")
} else if(n_models <= 12) {
  colors <- RColorBrewer::brewer.pal(12, "Set3")
} else {
  colors <- rainbow(n_models, s = 0.7, v = 0.8)  # More muted rainbow
}

# Ensure we have enough colors
if(length(colors) < n_models) {
  colors <- rep(colors, ceiling(n_models/length(colors)))[1:n_models]
}

# Initialize empty plot
p <- plot_ly()

# Add a trace for each model with different colors
for(i in 1:length(unique_models)) {
  model <- unique_models[i]
  model_data <- df_plot %>% filter(model_name == model)
  model_stat <- model_stats %>% filter(model_name == model)
  
  # Create custom hover template with statistics
  hover_template <- paste0(
    "<b>Model:</b> %{x}<br>",
    "<b>Value:</b> %{y}<br>",
    "<b>Statistics:</b><br>",
    "Mean: ", model_stat$mean, "<br>",
    "Median: ", model_stat$median, "<br>",
    "Q1: ", model_stat$q1, "<br>",
    "Q3: ", model_stat$q3, "<br>",
    "IQR: ", model_stat$iqr, "<br>",
    "Min: ", model_stat$min_val, "<br>",
    "Max: ", model_stat$max_val,
    "<extra></extra>"
  )
  
  p <- p %>% add_trace(
    data = model_data,
    x = ~model_name,
    y = ~avg_score,
    type = "box",
    name = model,
    boxpoints = "outliers",
    marker = list(
      size = 4,
      opacity = 0.7,
      color = colors[i]
    ),
    line = list(
      width = 2,
      color = colors[i]
    ),
    fillcolor = paste0(colors[i], "60"),  # Add transparency
    boxmean = FALSE,
    notched = TRUE,
    jitter = 0.3,
    pointpos = 0,
    hovertemplate = hover_template,
    hoverlabel = list(
      bgcolor = colors[i],
      font = list(color = "white", size = 12)
    )
  )
}

# Customize layout with enhanced styling
p <- p %>% layout(
  title = list(
    text = 'BERT F1-Score Distribution by Model',
    font = list(size = 20, family = "Arial Black"),
    x = 0.5,
    y = 0.95
  ),
  
  xaxis = list(
    title = list(
      text = '',
      font = list(size = 14, family = "Arial")
    ),
    tickfont = list(size = 11, family = "Arial"),
    tickangle = 45,
    showgrid = TRUE,
    gridwidth = 1,
    gridcolor = 'rgba(128,128,128,0.2)',
    zeroline = FALSE,
    showline = TRUE,
    linewidth = 2,
    linecolor = 'rgba(128,128,128,0.8)',
    type = 'category'  # Ensures only model names appear on x-axis
  ),
  
  yaxis = list(
    title = list(
      text = 'BERT f1 Score',  # Updated y-axis title
      font = list(size = 14, family = "Arial")
    ),
    tickfont = list(size = 11, family = "Arial"),
    showgrid = TRUE,
    gridwidth = 1,
    gridcolor = 'rgba(128,128,128,0.2)',
    zeroline = TRUE,
    zerolinewidth = 2,
    zerolinecolor = 'rgba(128,128,128,0.4)',
    showline = TRUE,  # Fixed: was "True" instead of TRUE
    linewidth = 2,
    linecolor = 'rgba(128,128,128,0.8)'
  ),
  
  plot_bgcolor = '#fafafa',
  paper_bgcolor = 'white',
  
  showlegend = FALSE,
  
  margin = list(
    l = 80,
    r = 60,
    b = 120,
    t = 100
  )
)

# Display the plot
p

```

Here, we can see that Claude 3.5 is slightly better than Llama 4. However, the distribution for most LLMs is certainly wide, showing lack of consistency across all questions. To investigate this further, a heatmap was constructed and is shown in the figure below. Here, we can clearly see the variability across different questions and how GPT-4 consistently underperforms for this particular task. The questions itself and the descriptions of each question can be found below the graph.


```{r message=FALSE, warning=FALSE}
#| out-width: "100%"

heatmap_data <- df_plot %>%
  group_by(question, model_name) %>%
  summarise(avg_score = mean(avg_score, na.rm = TRUE), .groups = 'drop') %>%
  pivot_wider(names_from = model_name, values_from = avg_score)

# Convert to matrix for easier handling
questions <- heatmap_data$question
heatmap_matrix <- as.matrix(heatmap_data[, -1])

# Create integer labels and mapping
integer_labels <- 1:length(questions)
rownames(heatmap_matrix) <- integer_labels

# Calculate center value for color scale
center_value <- mean(heatmap_matrix, na.rm = TRUE)

# Create custom color scale (equivalent to RdYlBu_r)
colors <- c('#d73027', '#f46d43', '#fdae61', '#fee08b', '#ffffbf', 
            '#e6f598', '#abdda4', '#66c2a5', '#3288bd', '#5e4fa2')

# Create the plotly heatmap
p <- plot_ly(
  z = ~heatmap_matrix,
  x = ~colnames(heatmap_matrix),
  y = ~integer_labels,  # Use integer labels
  type = "heatmap",
  colorscale = list(
    c(0, colors[1]), c(0.1, colors[2]), c(0.2, colors[3]), c(0.3, colors[4]),
    c(0.4, colors[5]), c(0.6, colors[6]), c(0.7, colors[7]), c(0.8, colors[8]),
    c(0.9, colors[9]), c(1, colors[10])
  ),
  zmid = center_value,
  text = ~round(heatmap_matrix, 2),
  texttemplate = "%{text}",
  textfont = list(color = "black"),
  showscale = TRUE,
  colorbar = list(
    title = list(text = "BERT F1-Score", side = "right"),
    thickness = 20,
    len = 0.8
  )
) %>%
layout(
  title = list(
    text = "Model Performance by Question"
  ),
  xaxis = list(
    title = "",
    tickangle = 45,
    side = "bottom"
  ),
  yaxis = list(
    title = "Question",
    tickmode = "array",
    tickvals = integer_labels,
    ticktext = integer_labels,
    automargin = TRUE,
    autorange = "reversed"  # Show integers 1, 2, 3... from top to bottom
  ),

  autosize = TRUE
)
p
```

1. **ADDITIONAL FINDINGS** \- Documents other pathologic findings in the specimen beyond the primary tumor, such as atypical adenomatous hyperplasia, granulomatous inflammation, or emphysema.  
2. **CLOSEST MARGIN(S) TO INVASIVE CARCINOMA** \- Identifies which specific surgical margin (bronchial, vascular, parenchymal, or chest wall) is nearest to the invasive tumor.  
3. **DISTANCE FROM INVASIVE CARCINOMA TO CLOSEST MARGIN** \- Measures in centimeters how far the invasive tumor extends from the nearest surgical margin.  
4. **HISTOLOGIC GRADE** \- Assesses the degree of tumor differentiation using grading schemes specific to tumor type (G1-well differentiated to G4-undifferentiated).  
5. **LYMPH NODE(S) FROM PRIOR PROCEDURES** \- Documents whether lymph nodes from previous surgical procedures are included in the current specimen.  
6. **LYMPHOVASCULAR INVASION** \- Reports the presence of tumor cells within lymphatic vessels, arteries, or veins.  
7. **MARGIN STATUS FOR INVASIVE CARCINOMA** \- Determines whether invasive tumor is present at any surgical margin or if all margins are negative.  
8. **MARGIN STATUS FOR NON-INVASIVE TUMOR** \- Assesses whether carcinoma in situ or lepidic components are present at surgical margins.  
9. **NODAL SITE(S) EXAMINED** \- Documents which specific lymph node stations according to the IASLC map were sampled and examined.  
10. **NODAL SITE(S) WITH TUMOR** \- Identifies which specific lymph node stations contain metastatic tumor.  
11. **NUMBER OF LYMPH NODES EXAMINED** \- Provides the total count of lymph nodes examined in the specimen.  
12. **NUMBER OF LYMPH NODES WITH TUMOR** \- Reports the count of lymph nodes containing metastatic tumor.  
13. **PROCEDURE** \- Specifies the type of surgical resection performed (wedge resection, lobectomy, pneumonectomy, etc.).  
14. **REGIONAL LYMPH NODE STATUS** \- Provides overall assessment of whether regional lymph nodes contain tumor or are negative.  
15. **SPECIMEN LATERALITY** \- Indicates whether the lung specimen is from the right or left side.  
16. **SPREAD THROUGH AIR SPACES (STAS)** \- Documents the presence of tumor cells extending beyond the main tumor into surrounding air spaces as micropapillary clusters, solid nests, or single cells.  
17. **TNM DESCRIPTORS** \- Assigns pathologic TNM staging categories (pT, pN, pM) based on tumor size, nodal involvement, and distant metastasis.  
18. **TOTAL TUMOR SIZE (SIZE OF ENTIRE TUMOR)** \- Measures the greatest dimension of the entire tumor including both invasive and non-invasive components.  
19. **TUMOR FOCALITY** \- Determines whether there is a single tumor focus, multiple separate nodules, or multifocal disease.  
20. **TUMOR SITE** \- Identifies the specific anatomic location of the tumor within the lung (upper lobe, lower lobe, bronchus, etc.).  
21. **VISCERAL PLEURA INVASION** \- Assesses whether tumor penetrates beyond the elastic layer of the visceral pleura or extends to the pleural surface.

Further investigation is required to evaluate and characterize the failure modes of each of the LLMs. Additionally, the baseline performance indicates that fine-tuning an open-source model like Llama 4 may yield better results.