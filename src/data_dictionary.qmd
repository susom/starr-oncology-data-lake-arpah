---
title: "Data Dictionary"
execute:
  echo: false
---

### OMOP-CDM

This document provides a comprehensive overview of the OMOP Common Data Model (CDM) tables available in the initial release of the dataset. The OMOP-CDM is a standardized data model designed to facilitate the analysis and sharing of healthcare data across different institutions and studies. By using a common structure and terminology, the OMOP-CDM enables researchers to perform large-scale observational research and generate real-world evidence.

In this data dictionary, you will find detailed descriptions of each table and its columns, including data types, requirements, and any specific operations applied to the data. This information is crucial for understanding the structure and content of the dataset, ensuring accurate and meaningful analysis.

#### List of OMOP-CDM tables available

The following OMOP-CDM tables are available in the initial release of dataset:

| Category            | Table Name                                                                                   |
|---------------------|----------------------------------------------------------------------------------------------|
| Clinical Data       | [condition_occurrence](omop_data_dict/condition_occurrence.qmd), [drug_exposure](omop_data_dict/drug_exposure.qmd), [device_exposure](omop_data_dict/device_exposure.qmd), [measurement](omop_data_dict/measurement.qmd), [observation](omop_data_dict/observation.qmd), [procedure_occurrence](omop_data_dict/procedure_occurrence.qmd), [image_occurrence](omop_data_dict/image_occurrence.qmd),[visit_occurrence](omop_data_dict/visit_occurrence.qmd), [visit_detail](omop_data_dict/visit_detail.qmd), [note](omop_data_dict/note.qmd), [death](omop_data_dict/death.qmd) |
| Vocabularies        | [concept](omop_data_dict/concept.qmd), [concept_ancestor](omop_data_dict/concept_ancestor.qmd), [concept_class](omop_data_dict/concept_class.qmd), [concept_relationship](omop_data_dict/concept_relationship.qmd), [concept_synonym](omop_data_dict/concept_synonym.qmd), [domain](omop_data_dict/domain.qmd), [relationship](omop_data_dict/relationship.qmd), [vocabulary](omop_data_dict/vocabulary.qmd) |
| Health System Data  | [care_site](omop_data_dict/care_site.qmd), [location](omop_data_dict/location.qmd), [provider](omop_data_dict/provider.qmd) |
| Derived Elements    | [condition_era](omop_data_dict/condition_era.qmd), [drug_era](omop_data_dict/drug_era.qmd) |
| Health Economics    | [payer_plan_period](omop_data_dict/payer_plan_period.qmd) |
| Metadata            | [cdm_source](omop_data_dict/cdm_source.qmd), [metadata](omop_data_dict/metadata.qmd) |
:  {.hover .responsive .sm}


### NeuralFrame 

The following NeuralFrame tables are available in the release of the dataset:

* [Case Diagnoses](neuralframe_data_dict/case_diagnoses.qmd)
* [Misc Details](neuralframe_data_dict/case_misc_details.qmd)
* [Case Outcomes](neuralframe_data_dict/case_outcomes.qmd)
* [Case Treatments](neuralframe_data_dict/case_treatments.qmd)

### Phillips ISPM

The following tables are available in the initial release of the dataset:

* [Philips ISPM Aberration](philips_ispm_data_dict/aberrations.qmd)
* [Philips ISPM Orders](philips_ispm_data_dict/pat_diag_orders.qmd)
* [Philips ISPM Specimen](philips_ispm_data_dict/specimen_data.qmd)

### STAMP Add-On

* [STAMP Add On Data](stamp_add_on_data_dict/stamp_addon_data.qmd)

### Epic Media Server Documents 

* [Epic Media Server Documents](epic_media_server_dict/epic_media_server.qmd)

### HIPPO Benchmark

HIPPO stands for Human-in-the-loop Identification of PHI with Preserved Output. This internal fully identified dataset consists of two tables:

* [Gold Standard Full Text](hippo_data_dict/phi_dataset.qmd#full-text): This table contains the clinical text from which the annotations were gathered. It also contains basic demographic information about the patients to whom the notes refer.

* [Gold Standard Spans](hippo_data_dict/phi_dataset.qmd#spans): This table contains the annotated spans where PHI was identified, along with the corresponding labels and the note identifier to which the spans correspond.

### Lung Resection CAP Forms

This dataset is composed by two groups of tables and one cleaned dataset in QA format:

* [EPIC Clarity tables](cap_forms_data_dict/clarity_tables.qmd): These are the source EPIC Clarity tables required to assemble the information contained in the CAP forms as well as the pathology reports. The description of those   
* [Processed tables](cap_forms_data_dict/processed_tables.qmd): These are two tables: `cap_forms` and `shc_pathology_reports`. The first table contains the structured information with the CAP forms. The second table contains the pathology reports with the individual elements that compose them each one separated. 
* [QA cleaned dataset](cap_forms_data_dict/qa_dataset.qmd): This is a QA dataset where the questions, answers and context and clearly defined. This dataset is formatted as a HuggingFace dataset and can be loaded directly for training. 

### PHI Scrubbing Operations Definitions

In order to protect patient information, various PHI (Protected Health Information) scrubbing operations are applied to the dataset. These operations are designed to scrub sensitive data elements. Below are the definitions of the PHI scrubbing operations used in this dataset:

| PHi-Scrubbing Operation Name | Description                                                                                                     |
|------------------------------|-----------------------------------------------------------------------------------------------------------------|
| Not Stable between Data Refreshes                       | Not Stable between Data Refreshes specific to the project and dataset column                                                               |
| Jitter                       | Jitter specific to the project and dataset column                                                               |
| Del                          | Delete the values in this Column                                                                                |
| Sub                          | Substitute with a value specific to the project and dataset column                                              |
| Whitelist                    | Only allow the values in the 'allowed list' table to pass through; otherwise replace with NULL                  |
| Hash                         | Replace with a hash value based on a hashing function specific to the project and dataset column                |
| TiDE                         | Pass through [TiDE](https://starr.stanford.edu/methods/tide-clinical-text-safe-harbor), Stanford's text PHI scrubbing algorithm                                                      |
| RedZip                       | Reduce the Zipcode precision based on Stanford's University Privacy Office guidelines as outlined [here](https://docs.google.com/document/d/1wC_ikzVahKUQuGSod4OfFKVPfedAnqO-tDHjo4O2ygY/edit?tab=t.0), Sec 1.4 |
| Drop                         | Drop this column in the PHI scrubbed dataset                                                                    |
| Stable Between Data Refreshes     | Substitute with a stable identifier specific to the project and dataset column that will persist between data refreshes. This may be an integer or alphanumeric based on other  requirements of the table |
| Not Stable between Data Refreshes | Substitute with an identifier that is only specific and self consistent for that data refresh -could be an integer or alphanumeric based on  requirements of the table                                    |
| None                              | pass as is- no PHI scrubbing operation                                                                                                                                                                    |

:  {.hover .responsive .sm}

#### DICOM PHI Scrubbing

For DICOM data the PHI scrubbing module protects PII and sensitive information through a two-stage process. First, it uses regex patterns to identify and mask specific data types including dates, times, email addresses, URLs, numeric identifiers, and hexadecimal sequences. Second, it tokenizes the text using configurable delimiters and numeric-to-alpha and alpha-to-numeric transitions, then replaces any tokens not found in a predefined allowlist with 'X' characters of equivalent length. The system preserves the original text structure by maintaining spacing and delimiters while systematically redacting content. It includes specialized date handling through regex patterns that match various date formats, ensuring that only approved vocabulary passes through unredacted while obscuring all other potentially sensitive information.

This redaction only applies to `series_description`, `study_description`, and `protocol_name` attributes (tags).